{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "498edcd6",
   "metadata": {},
   "source": [
    "# Lecture 1: An Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "513a8d5e",
   "metadata": {},
   "source": [
    "## Basic Concepts:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eb19c5b",
   "metadata": {},
   "source": [
    "### Two Purposes of Statistical Inference:\n",
    "- **Prediction:**\n",
    "    - Purpose: \n",
    "        - Given X (features/independent variables) what is our best estimate of outcome Y (taregt/dependent variable)?\n",
    "    - Characteristic:\n",
    "        - Minimize some sort of expected prediction error (using in-sample prediction error on Y as an approximate)\n",
    "        - Care and evaluate out-of-sample performance (is there overfitting? is the prediction stable?)\n",
    "- **Causal Inference:**\n",
    "    - Purpose: \n",
    "        - What happens to Y (treatment effect) if we intervene to change some X while keeping other X unchanged?\n",
    "        - In other word, predicting (estimating) the treatment effect (TE)\n",
    "    - Characteristic:\n",
    "        - Impossible to minimize expected prediction error as the true TE is never observed\n",
    "        - As a result, need variations that isolates causal mechanism in order to identify the TE estimator\n",
    "            - For experimental data, such variatrion is guaranteed through random assignment of treatment\n",
    "            - For observational data, however, the variation is achieved through quasi-experiment or assumptions\n",
    "        - Care out-of-sample performance (is the TE estimator stable?) but cannot evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f82d19a",
   "metadata": {},
   "source": [
    "### Two Cultures of Statistical Inference:\n",
    "- **Parametric Methods:**\n",
    "    - Assumes that the data are generated by a given stochastic data model / data generation process (DGP)\n",
    "    - DGP: an assumed relationship between Y and X plus a stochastic error term\n",
    "- **Non-parametric Methods:**\n",
    "    - Uses algorithmic models and treats the data generation process as unknown\n",
    "    - These methods are usually referred as machine learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae581841",
   "metadata": {},
   "source": [
    "### Machine Learning Revolution:\n",
    "- The traditional statistics adopted parametric methods for prediction.\n",
    "- Machine learning (non-parametric methods) revolutize this field through three advantages:\n",
    "    - ML concentrated on developing computationally efficient algorithms\n",
    "    - Through introudcing regularization, many ML methods leverage the variance-bias tradeoff to tackle prediction problem in high dimension\n",
    "    - ML algorithms are model agnostic; they do not pre-assume a function form of relation between feature and target\n",
    "- Big data challenge and mean-variance trade-ff\n",
    "    - For big data, variable number is usually large relative to the size of observations (high dimension problem)\n",
    "    - Most parametric methods will result in high variance on estimators in this scenario (reduce out-sample prediction accuracy)\n",
    "    - Through introudcing regularization, ML methods drop some variables to reduce variance of the estimators\n",
    "    - This imporves out-of-sample stability of the estimator (and prediction) while increasing bias (the estimator is no longer correct in average)\n",
    "    - That's why it is called variance-bias trade-off"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebd606d8",
   "metadata": {},
   "source": [
    "### Machine Learning and Econometrics:\n",
    "- The adoption of ML methods in econometrics has been slower, as the goal of econometrics is causal inference, while ML intend to optimize prediction\n",
    "- However, there is a way to combine machine learning (non-parametric methods) with causal inference, which is causal machine learning\n",
    "- **Benefit of causal ML:**\n",
    "    - ML methods do not create quasi-experimental variation\n",
    "    - That is, if the identification assumption for causal inference (in RCT, conditional independence, DiD, instrument variable etc.) do not hold, causal ML will not help\n",
    "    - Instead, causal ML methods provide tools for robustness checking and efficiency improvement on TE estimators (lower variance)\n",
    "    - For example, we may find TE identified using parametric methods with control for selection and confounding effects not different from 0 at 95% CI\n",
    "    - Causal ML might overtun the conclusion by further restrict the CI boundary (but no guarantee)\n",
    "- **Challenge of causal ML:**\n",
    "    - Causal inference are based on identification assumptions\n",
    "    - These assumptions need to be carrired over to the use of ML to causal inference\n",
    "    - This have created challenges for the modification of predictive algorithms for causal inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3a6ded1",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Causal ML: Example on LASSO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7431596e",
   "metadata": {},
   "source": [
    "### Set Up:\n",
    "- Suppose we want to estimate the effect of a treatment $d$ on $Y$ with confounders $X$\n",
    "- $X$ is a vector of $P$ values, with $P$ very close to number of observation $N$\n",
    "- The condition independece assumption holds: after controlling $X$, the assignment of $d$ can be treated as random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a75b77a8",
   "metadata": {},
   "source": [
    "### OLS:\n",
    "- OLS equation: $Y = \\tau d + X \\beta + \\epsilon$\n",
    "- Due to the high dimension of $X$, our estimator on TE $\\hat \\tau$ using OLS will be very unstable (though unbiased)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5087fb49",
   "metadata": {},
   "source": [
    "### How LASSO Can Help:\n",
    "- If many coefficients correpsonding to variables in $X$ are 0 or close to 0, we say that we have approximate sparsity on the regression coefficients\n",
    "- In this case, we can use LASSO to shrink some coefficients on controls to 0 to reduce dimensionality\n",
    "- In turn, this will perform well in prediction task, as LASSO conducts feature selection and reduce variance on estimators\n",
    "\n",
    "- **Can we apply LASSO for causal inference?** :\n",
    "    - LASSO shrink size of estimators, this will make estimator on TE $\\hat \\tau$ invalid\n",
    "- **Solution: Naive Post LASSO**:\n",
    "    - Estimate the equation $Y = \\tau d + X \\beta + \\epsilon$ first with LASSO penalty terms\n",
    "    - Then re-estimate the equation with variables that have estimated coefficient not equal to 0 under LASSO\n",
    "    - To prevent $d$ being dropped (possible if it is correlated with controls), we do not include $\\tau$ in the LASSO penalty\n",
    "\n",
    "- **Problem: Regularisation bias** : \n",
    "    - Controls that are strongly correlated with $d$ and having little effect on $Y$ will be dropped\n",
    "    - This introduces the omitted variable bias, contaminating the TE estimator $\\hat \\tau$ in the second stage OLS\n",
    "- **Solution: Double LASSO**:\n",
    "    - Estimate the equation $Y = X \\beta + \\epsilon$ first with LASSO penalty terms\n",
    "    - Derive residual $\\tilde Y = Y - X \\hat \\beta _ Y $\n",
    "    - Estimate the equation $d = X \\beta + \\epsilon$ next with LASSO penalty terms\n",
    "    - Derive residual $\\tilde d = d - X \\hat \\beta _ d $\n",
    "    - Estimate the equation $\\tilde Y = \\alpha + \\tau \\tilde d + \\epsilon$ with OLS\n",
    "    - By FWL Theorem and Neyman Orthogonality, $\\hat \\tau$ from the final OLS regression is an TE estimate robust to regularisation bias\n",
    "\n",
    "- **Problem: Overfitting bias** :\n",
    "    - If we pinned down the LASSO penalty size using cross validation on dataset $D$\n",
    "    - And train LASSO models and compute residualized variables $\\tilde Y$ and $\\tilde d$ using same dataset\n",
    "    - Then any overfitting at the training stage of LASSO will convert to bias at the parameter estimation stage of $\\hat \\tau$\n",
    "- **Solution: Sample Splitting**:\n",
    "    - Split $D$ to two sets $D_1$ and $D_2$\n",
    "    - Use $D_1$ for cross-validation and traning LASSO models \n",
    "    - Apply trained LASSO models on $D_2$ to get residualized variable $\\tilde Y$ and $\\tilde d$\n",
    "    - Estimate $\\hat \\tau$ from the final OLS regression on $\\tilde Y$ and $\\tilde d$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d82ee50",
   "metadata": {},
   "source": [
    "## "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
