{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "498edcd6",
   "metadata": {},
   "source": [
    "# Lecture 1: An Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "513a8d5e",
   "metadata": {},
   "source": [
    "## Basic Concepts:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eb19c5b",
   "metadata": {},
   "source": [
    "### Two Purposes of Statistical Inference:\n",
    "- **Prediction:**\n",
    "    - Purpose: \n",
    "        - Given X (features/independent variables) what is our best estimate of outcome Y (taregt/dependent variable)?\n",
    "    - Characteristic:\n",
    "        - Minimize some sort of expected prediction error (using in-sample prediction error on Y as an approximate)\n",
    "        - Care and evaluate out-of-sample performance (is there overfitting? is the prediction stable?)\n",
    "- **Causal Inference:**\n",
    "    - Purpose: \n",
    "        - What happens to Y (treatment effect) if we intervene to change some X while keeping other X unchanged?\n",
    "        - In other word, predicting (estimating) the treatment effect (TE)\n",
    "    - Characteristic:\n",
    "        - Impossible to minimize expected prediction error as the true TE is never observed\n",
    "        - As a result, need variations that isolates causal mechanism in order to identify the TE estimator\n",
    "            - For experimental data, such variatrion is guaranteed through random assignment of treatment\n",
    "            - For observational data, however, need variation achieved through quasi-experiment and assumptions\n",
    "        - Care out-of-sample performance (is the TE estimator stable?) but cannot evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f82d19a",
   "metadata": {},
   "source": [
    "### Two Cultures of Statistical Inference:\n",
    "- **Parametric Methods:**\n",
    "    - Assumes that the data are generated by a given stochastic data model / data generation process (DGP)\n",
    "    - DGP: an assumed relationship between Y and X plus a stochastic error term\n",
    "- **Non-parametric Methods:**\n",
    "    - Uses algorithmic models and treats the data generation process as unknown\n",
    "    - These methods are usually referred as machine learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae581841",
   "metadata": {},
   "source": [
    "### Big Data Challenge:\n",
    "- For big data, variable number is usually large relative to the size of observations\n",
    "- This feature makes parametric methods less efficient on prediction tasks and induces the ML revolution\n",
    "- Advantage of Machine learning (non-parametric methods) relative to parametric methods:\n",
    "    - ML concentrated on developing computationally efficient predictive algorithms\n",
    "    - It leverages the biasâ€“variance trade-off in order to achieve high predictive accuracy\n",
    "    - Through introudcing regularization, many ML methods drop some variables to reduce variance of the estimators\n",
    "    - This imporves out-of-sample stability of the estimator (and prediction) while increasing bias (the estimator is no longer correct in average)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
