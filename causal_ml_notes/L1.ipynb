{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "498edcd6",
   "metadata": {},
   "source": [
    "# Lecture 1: An Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "513a8d5e",
   "metadata": {},
   "source": [
    "## Basic Concepts:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eb19c5b",
   "metadata": {},
   "source": [
    "### Two Purposes of Statistical Inference:\n",
    "- **Prediction:**\n",
    "    - Purpose: \n",
    "        - Given X (features/independent variables) what is our best estimate of outcome Y (taregt/dependent variable)?\n",
    "    - Characteristic:\n",
    "        - Minimize some sort of expected prediction error (using in-sample prediction error on Y as an approximate)\n",
    "        - Care and evaluate out-of-sample performance (is there overfitting? is the prediction stable?)\n",
    "- **Causal Inference:**\n",
    "    - Purpose: \n",
    "        - What happens to Y (treatment effect) if we intervene to change some X while keeping other X unchanged?\n",
    "        - In other word, predicting (estimating) the treatment effect (TE)\n",
    "    - Characteristic:\n",
    "        - Impossible to minimize expected prediction error as the true TE is never observed\n",
    "        - As a result, need variations that isolates causal mechanism in order to identify the TE estimator\n",
    "            - For experimental data, such variatrion is guaranteed through random assignment of treatment\n",
    "            - For observational data, however, the variation is achieved through quasi-experiment or assumptions\n",
    "        - Care out-of-sample performance (is the TE estimator stable?) but cannot evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f82d19a",
   "metadata": {},
   "source": [
    "### Two Cultures of Statistical Inference:\n",
    "- **Parametric Methods:**\n",
    "    - Assumes that the data are generated by a given stochastic data model / data generation process (DGP)\n",
    "    - DGP: an assumed relationship between Y and X plus a stochastic error term\n",
    "- **Non-parametric Methods:**\n",
    "    - Uses algorithmic models and treats the data generation process as unknown\n",
    "    - These methods are usually referred as machine learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae581841",
   "metadata": {},
   "source": [
    "### Machine Learning Revolution:\n",
    "- The traditional statistics adopted parametric methods for prediction.\n",
    "- Machine learning (non-parametric methods) revolutize this field through three advantages:\n",
    "    - ML concentrated on developing computationally efficient algorithms\n",
    "    - Through introudcing regularization, many ML methods leverage the variance-bias tradeoff to tackle prediction problem in high dimension\n",
    "    - ML algorithms are model agnostic; they do not pre-assume a function form of relation between feature and target\n",
    "- Big data challenge and mean-variance trade-ff\n",
    "    - For big data, variable number is usually large relative to the size of observations (high dimension problem)\n",
    "    - Most parametric methods will result in high variance on estimators in this scenario (reduce out-sample prediction accuracy)\n",
    "    - Through introudcing regularization, ML methods drop some variables to reduce variance of the estimators\n",
    "    - This imporves out-of-sample stability of the estimator (and prediction) while increasing bias (the estimator is no longer correct in average)\n",
    "    - That's why it is called variance-bias trade-off"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebd606d8",
   "metadata": {},
   "source": [
    "### Machine Learning and Econometrics:\n",
    "- The adoption of ML methods in econometrics has been slower, as the goal of econometrics is causal inference, while ML intend to optimize prediction\n",
    "- However, there is a way to combine machine learning (non-parametric methods) with causal inference, which is causal machine learning\n",
    "- **Benefit of causal ML:**\n",
    "    - ML methods do not create quasi-experimental variation\n",
    "    - That is, if the identification assumption for causal inference (in RCT, conditional independence, DiD, instrument variable etc.) do not hold, causal ML will not help\n",
    "    - Instead, causal ML methods provide tools for robustness checking and efficiency improvement on TE estimators (lower variance)\n",
    "    - For example, we may find TE identified using parametric methods with control for selection and confounding effects not different from 0 at 95% CI\n",
    "    - Causal ML might overtun the conclusion by further restrict the CI boundary (but no guarantee)\n",
    "- **Challenge of causal ML:**\n",
    "    - Causal inference are based on identification assumptions\n",
    "    - These assumptions need to be carrired over to the use of ML to causal inference\n",
    "    - This have created challenges for the modification of predictive algorithms for causal inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3a6ded1",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Causal ML: Example on LASSO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7431596e",
   "metadata": {},
   "source": [
    "### Set Up:\n",
    "- Suppose we want to estimate the effect of a treatment $d$ on $Y$ with confounders $X$\n",
    "- $X$ is a vector of $P$ values, with $P$ very close to number of observation $N$\n",
    "- The condition independece assumption holds: after controlling $X$, the assignment of $d$ can be treated as random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a75b77a8",
   "metadata": {},
   "source": [
    "### OLS:\n",
    "- OLS equation: $Y = \\tau d + X \\beta + \\epsilon$\n",
    "- Due to the high dimension of $X$, our estimator on TE $\\hat \\tau$ using OLS will be very unstable (though unbiased)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5087fb49",
   "metadata": {},
   "source": [
    "### How LASSO Can Help:\n",
    "- If many coefficients correpsonding to variables in $X$ are 0 or close to 0, we say that we have approximate sparsity on the regression coefficients\n",
    "- In this case, we can use LASSO to shrink some coefficients on controls to 0 to reduce dimensionality\n",
    "- In turn, this will perform well in prediction task, as LASSO conducts feature selection and reduce variance on estimators\n",
    "\n",
    "- **Can we apply LASSO for causal inference?** :\n",
    "    - LASSO shrink size of estimators, this will make estimator on TE $\\hat \\tau$ invalid\n",
    "- **Solution: Naive Post LASSO**:\n",
    "    - Estimate the equation $Y = \\tau d + X \\beta + \\epsilon$ first with LASSO penalty terms\n",
    "    - Then re-estimate the equation with variables that have estimated coefficient not equal to 0 under LASSO\n",
    "    - This approach reduces the shrinkage bias while maintaining the benefits of variable selection\n",
    "    - To prevent $d$ being dropped (possible if it is correlated with controls), we do not include $\\tau$ in the LASSO penalty\n",
    "\n",
    "- **Problem: Regularisation bias** : \n",
    "    - Controls that are strongly correlated with $d$ and having little effect on $Y$ will be dropped\n",
    "    - This introduces the omitted variable bias, contaminating the TE estimator $\\hat \\tau$ in the second stage OLS\n",
    "- **Solution: Double LASSO**:\n",
    "    - Estimate the equation $Y = X \\beta + \\epsilon$ first with LASSO penalty terms\n",
    "    - Derive residual $\\tilde Y = Y - X \\hat \\beta _ Y $\n",
    "    - Estimate the equation $d = X \\beta + \\epsilon$ next with LASSO penalty terms\n",
    "    - Derive residual $\\tilde d = d - X \\hat \\beta _ d $\n",
    "    - Estimate the equation $\\tilde Y = \\alpha + \\tau \\tilde d + \\epsilon$ with OLS\n",
    "    - By FWL Theorem and Neyman Orthogonality, $\\hat \\tau$ from the final OLS regression is an TE estimate robust to regularisation bias\n",
    "\n",
    "- **Problem: Overfitting bias** :\n",
    "    - If we pinned down the LASSO penalty size using cross validation on dataset $D$\n",
    "    - And train LASSO models and compute residualized variables $\\tilde Y$ and $\\tilde d$ using same dataset\n",
    "    - Then any overfitting at the training stage of LASSO will convert to bias at the parameter estimation stage of $\\hat \\tau$\n",
    "- **Solution: Sample Splitting**:\n",
    "    - Split $D$ to two sets $D_1$ and $D_2$\n",
    "    - Use $D_1$ for cross-validation and traning LASSO models \n",
    "    - Apply trained LASSO models on $D_2$ to get residualized variable $\\tilde Y$ and $\\tilde d$\n",
    "    - Estimate $\\hat \\tau$ from the final OLS regression on $\\tilde Y$ and $\\tilde d$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d39302c",
   "metadata": {},
   "source": [
    "## Generalised Double Debiased ML "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d82ee50",
   "metadata": {},
   "source": [
    "### Basic Framework\n",
    "- Set Up:\n",
    "    - the dependent variable is $Y$\n",
    "    - set of confounders: $X$\n",
    "    - treatment indicator: $d$\n",
    "    - the structural form (real relationship) between $Y$, $X$, and $d$ is: \n",
    "        - $Y_i = \\tau d_i + g(X_i) + \\epsilon _i$ (partial linear model: Y linear in d but non-linear in X)\n",
    "        - $d_i = m(X_i) + v _i$ (partial linear model: Y linear in d but non-linear in X)\n",
    "- Sample Splitting: \n",
    "    - split the dataset intwo two pieces $D_1$ and $D_2$\n",
    "    - one piece ($D_1$) for model selection (train non-parametric ML model for predicting $Y$ and $d$ using $X$)\n",
    "    - another piece ($D_2$) for causal inference parameter estimation\n",
    "- Prediction (Selection) Stage:\n",
    "    - find how confounders relate to outcomes and treatments (nuisance function estimation):\n",
    "        - $g_0(X_i) = E[Y_i|X_i]$\n",
    "        - $m_0(X_i) = E[d_i|X_i]$\n",
    "        - where $g_0(.)$ and $m_0(.)$ is estimated using nonparametric ML methods\n",
    "        - their estimates are $\\hat g_0(.)$ and $\\hat m_0(.)$\n",
    "        - their form + confounders included is \"selected\" though ML methods\n",
    "        - note: nuisance functions affect identification of the treatment effect, but their parameters are not of intrinsic interest\n",
    "- Causal Inference Stage:\n",
    "    - assuming that the condition $E[(Y_i − \\tau d_i − g(X_i))(d_i − m(X_i))] = 0$ holds at true paremeters $\\tau$, $g$, and $m$\n",
    "    - then by FWL theorem, $\\tau = \\frac{cov(Y_i^*, d_i^*)}{var(d_i^*)}$\n",
    "    - where $Y_i^* = Y_i − g_0(X_i)$ and $d_i^* = d_i − m_0(X_i)$\n",
    "    - this motivates a partialling-out (residualization) estimator, where:\n",
    "        - $\\tilde Y_i = Y_i − \\hat g_0(X_i)$\n",
    "        - $\\tilde d_i = d_i − \\hat m_0(X_i)$\n",
    "        - residuals are constructed on $D_2$ using nuisance models estimated on $D_1$\n",
    "        - TE $\\hat \\tau$ is estimated from OLS regression $\\tilde Y_i = \\tau \\tilde d_i + w_i$\n",
    "    - the scoring function of residualization estimator is Neyman-orthogonal, implying that $\\hat \\tau$ is locally insensitive to first-order errors in nuisance parameters ($g_0(.)$ and $m_0(.)$)\n",
    "- Two Sources of Bias Addressed:\n",
    "    - Regularisation Bias: \n",
    "        - the bias from using ML method for selection (e.g. drop some variables) becomes ignorable (insensitive to first-order errors in nuisance parameters)\n",
    "    - Overfitting Bias:\n",
    "        - we prevent from using same data for both nuisance estimation and TE parameter estimation\n",
    "        - thus we omit this bias"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18293a7d",
   "metadata": {},
   "source": [
    "### Extension to Heterogeneous Treatment Effects\n",
    "- Linear form (parametric) model: $y = α + \\tau d + X'β + ε$\n",
    "- Partial linear form (nuisance function estimated using non-parametric method): $y = α + \\tau d + g(X) + ε$\n",
    "- Fully nonparametric form: $y = α(X) + \\tau(X)d + ε$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8392656c",
   "metadata": {},
   "source": [
    "## Points of Departure of Causal ML"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72902eea",
   "metadata": {},
   "source": [
    "### Conditional Expectation Function\n",
    "- The CEF captures all the predictive information about Y that is contained in X\n",
    "- The principle of all statiscal learning techniques is approximating the CEF\n",
    "- OLS find the best (optimal among linear unbiased estimators for in-sample prediction) linear approximation of the CEF\n",
    "- It is **best** only if:\n",
    "    - we believe the CEF is linear in X\n",
    "    - we require unbiaseness\n",
    "- If CEF is not linear, we should deviate from OLS to ML methods\n",
    "- If we do not care unbiaseness, we can also transfer to ML methods that trade bias for lower variance (achieve better out-of-sample prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "047f7545",
   "metadata": {},
   "source": [
    "### High Dimensional Methods in Statistics\n",
    "- When p (number of predictors) are large relative to n (number of observations), we face the high-dimensional challenge\n",
    "- This challenge can be rephrased as curse of dimensionalty: as p increases, the sample size required for learning increase expotentially\n",
    "- The OLS estimators will be unstable (having high variance) in this setting, achieving poor performance in out-of-sample prediction\n",
    "- ML methods can trade bias and variance, achieving better performance through:\n",
    "    - Regularisation (Lasso, Ridge, Elastic Net)\n",
    "    - Model selection and screening\n",
    "    - Dimension reduction techniques\n",
    "- More specifically, under sparsity assumption (a subset of coefficients are zero), regularisation discovers which coefficients matter (thus reducing dimension)\n",
    "- The better prediction performance by ML can in turn be converted to more efficient causal inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5a9364e",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### Frisch-Waugh-Lovell (FWL) Theorem\n",
    "- When we apply regularisation methods like LASSO to select variables from high-dimensional X\n",
    "- It results in regularization bias (difference between estimator and true parameter) as:\n",
    "    - Some variable being dropped, creating bias similar to omitted variable bias\n",
    "    - Penalty term shrinks all coefficients based on predictive considerations\n",
    "- Solution: Applying FWL Logic\n",
    "    - we can use high dimensional methods (such as LASSO) to estimate X-Y relationship and residualise Y\n",
    "    - we can use high dimensional methods to estimate X-d relationship and residualise d\n",
    "    - finally estimate the τ from residualised regression\n",
    "- The estimator is valid under the identification assumption that $E[(Y − \\tau d − g(X))(d − m(X))] = 0$ at true paremeters $\\tau$, $g$, and $m$\n",
    "- Besides, the estimator of τ from last step will be locally insensitive to first-order error in estimator of X-Y relationship and X-d relationship"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23768e0d",
   "metadata": {},
   "source": [
    "### Treatment Effects\n",
    "- We will review the treatment effects literature including:\n",
    "    - Identification assumptions of TE\n",
    "    - Heterogeneous treatment effects\n",
    "    - Parametric and nonparametric estimators of TE\n",
    "- Also connection to ML methods through:\n",
    "    - Double robust estimators\n",
    "    - Causal forests for heterogeneity\n",
    "    - Optimal policy learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "888d2ff0",
   "metadata": {},
   "source": [
    "### Machine Learning Methods for Prediction\n",
    "- Note: ML helps estimate nuisance functions but cannot solve identification problems\n",
    "- Core ML principles:\n",
    "    - Flexibility in functional forms\n",
    "    - Data-driven model selection\n",
    "    - Focus on out-of-sample performance\n",
    "- Key algorithms:\n",
    "    - Regularised regression (Lasso, Ridge)\n",
    "    - Tree-based methods, Random Forests, and Generalised Random Forests\n",
    "    - Neural networks and deep learning\n",
    "    - Ensemble methods and stacking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77e9f893",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Two Sets of Causal ML Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e202e40",
   "metadata": {},
   "source": [
    "### High-Dimensional Controls/Instruments Approach\n",
    "- Models with a large number of potential confounding variables or instruments\n",
    "- Employs ML to manage the curse of dimensionality (variable selection and dimension reduction)\n",
    "- Tackles bias due to model selection\n",
    "- e.g. Double Debiased Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f46164e",
   "metadata": {},
   "source": [
    "### Treatment Effect Heterogeneity Approach\n",
    "- Explores and identifies heterogeneous treatment effects (conditional average TE, group average TE)\n",
    "- Facilitates understanding of differential treatment effects across groups\n",
    "- Enables personalized treatment decisions and targeted policy implementation\n",
    "- Modified MSE for heterogeneous causal effects:\n",
    "    - MSE measures expected squared difference between estimator and true parameter\n",
    "    - Original MSE: $MSE(\\hat \\theta) = E[{(\\hat \\theta - \\theta)}^2] = {[Bias(\\hat \\theta)]}^2 + Var(\\hat \\theta)$\n",
    "    - Modified MSE: $MSE(\\hat \\tau) = E[{(\\hat \\tau(X) - \\tau(X))}^2]$\n",
    "    - Key differences from standard MSE:\n",
    "        - Target is CATE not outcome prediction (e.g. Y), which is never observed\n",
    "        - Rewards finding genuine treatment effect heterogeneity\n",
    "- e.g. causal forest and generalized random forest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ee02337",
   "metadata": {},
   "source": [
    "## Extras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "413b630e",
   "metadata": {},
   "source": [
    "### Predictive Effect\n",
    "- Consider the regression model $Y = \\alpha + \\tau d + X \\beta + \\epsilon$\n",
    "- The regression coefficient $\\tau$ measures the linear prediction of Y if d changes from 0 to 1, holding the controls X\n",
    "- We can call this the **predictive effect** (PE), as it measures the impact of a variable on the prediction we make\n",
    "- PE is a measure of statistical dependence or association between d and Y even if we partial-out linearly the controls X\n",
    "- Without idetification assumptions, the PE should not be interpreted as a treatment effect (TE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc9b09d9",
   "metadata": {},
   "source": [
    "### Local to Global Treatment Effects\n",
    "- We can defining conditional average treatment effects as $\\tau (x) = E[Y(1) − Y(0)|X = x]$\n",
    "- ATE emerges from CATE through averaging: $\\tau _{ATE} = E[\\tau (x)] = E[E[Y(1) − Y(0)|X = x]]$\n",
    "- If CATE is constant, $\\tau (x) = \\tau$ and $\\tau _{ATE} = E[\\tau] = \\tau$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25c1fb67",
   "metadata": {},
   "source": [
    "### FWL Logic in Heterogeneous TE\n",
    "- For partial linear model, we residualized globally to derive the estimator of TE\n",
    "    - $Y_i^* = Y_i − g_0(X_i)$ and $d_i^* = d_i − m_0(X_i)$\n",
    "    - $\\tau = \\frac{cov(Y_i^*, d_i^*)}{var(d_i^*)}$\n",
    "- For heterogeneous TE (fully non-parametric model), residualisation must work locally, using either discrete or ’continuous’ partitions of X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6da4fbcf",
   "metadata": {},
   "source": [
    "### Classification Model\n",
    "- Econometrics commonly use parametric probit model for classification\n",
    "    - Probit model: $Y^* = X'\\beta + \\epsilon$ , $Y = 1 (Y^* > 0)$ , $E[Y|X] = Pr(\\epsilon < X'\\beta) = Φ(X'\\beta) $ under normality\n",
    "    - It is a normal CDF function applied to a linear combination of the predictors\n",
    "    - It makes global assumptions and estimate smooth probabilities\n",
    "    - Forbidden regression: we cannot use IV for Probit model (no IV for discrete dependent variable)\n",
    "- Classification trees:\n",
    "    - Partition the feature space into rectangular space\n",
    "    - Voting: estimate local probability as the fraction of training observations in that region belonging to each class\n",
    "    - This can capture non-linear relationships and interactions\n",
    "    - Can it circumvent the forbidden regression problem?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f7699c2",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### Model Interpretability\n",
    "- Black-box Approach:\n",
    "    - ML algorithms have historically been “black boxes”\n",
    "    - Difficult to understand their inner processes and explain to regulatory agencies and stakeholders\n",
    "    - Interpretability: the extent to which effect from each predictor can be determined\n",
    "- White-box approach:\n",
    "    - Decision making processes completely transparent (e.g. OLS, interpretable; poor prediction)\n",
    "    - Users able to audit decisions made by the model (e.g. gender should not contribute much to prediction)\n",
    "    - Answers to questions:\n",
    "        - Why the model made a particular decision?\n",
    "        - What are the most influencing variables for a particular decision?"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
